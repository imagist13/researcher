"""
ç ”ç©¶ä»»åŠ¡æ‰§è¡Œå™¨ - æ‰§è¡Œå…·ä½“çš„ç ”ç©¶ä»»åŠ¡
Research Task Executor - Executes specific research tasks
"""
import asyncio
import logging
import time
import json
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Any

from gpt_researcher.agent import GPTResearcher
from gpt_researcher.utils.enum import ReportType, ReportSource, Tone

from ..database import ResearchHistoryDAO, TrendDataDAO, TaskExecutionLogDAO
from ..server.content_safety import ContentSafetyChecker
from .trend_analyzer import TopicTrendAnalyzer
from .summary_generator import DynamicSummaryGenerator
from .config import ScheduledResearchConfig, ScheduledResearchPrompts

logger = logging.getLogger(__name__)


class ResearchTaskExecutor:
    """
    ç ”ç©¶ä»»åŠ¡æ‰§è¡Œå™¨
    Executes research tasks and manages the research workflow
    """
    
    def __init__(self, websocket_manager=None):
        """
        åˆå§‹åŒ–ä»»åŠ¡æ‰§è¡Œå™¨
        
        Args:
            websocket_manager: WebSocketç®¡ç†å™¨ï¼Œç”¨äºŽå®žæ—¶æŽ¨é€è¿›åº¦
        """
        self.websocket_manager = websocket_manager
        self.trend_analyzer = TopicTrendAnalyzer()
        self.summary_generator = DynamicSummaryGenerator()
    
    async def execute_task(self, task, execution_log_id: Optional[str] = None) -> Dict[str, Any]:
        """
        æ‰§è¡Œç ”ç©¶ä»»åŠ¡
        
        Args:
            task: ScheduledTaskå¯¹è±¡
            execution_log_id: æ‰§è¡Œæ—¥å¿—ID
            
        Returns:
            DictåŒ…å«æ‰§è¡Œç»“æžœ
        """
        start_time = time.time()
        result = {
            "success": False,
            "task_id": task.id,
            "execution_time": 0.0,
            "summary": "",
            "key_changes": [],
            "trend_score": 0.0,
            "sources_count": 0,
            "log_messages": []
        }
        
        try:
            logger.info(f"Starting research task execution: {task.topic}")
            
            # æ›´æ–°æ‰§è¡Œæ—¥å¿—
            await self._update_execution_log(execution_log_id, {
                "execution_step": "research_phase",
                "progress_percentage": 10.0,
                "log_messages": [{"timestamp": datetime.now().isoformat(), "message": "Starting research phase"}]
            })
            
            # ç¬¬ä¸€é˜¶æ®µï¼šæ‰§è¡Œç ”ç©¶
            research_result = await self._conduct_research(task)
            if not research_result["success"]:
                result["error"] = research_result.get("error", "Research failed")
                return result
            
            # æ›´æ–°æ‰§è¡Œæ—¥å¿—
            await self._update_execution_log(execution_log_id, {
                "execution_step": "analysis_phase",
                "progress_percentage": 60.0,
                "log_messages": [{"timestamp": datetime.now().isoformat(), "message": "Research completed, starting analysis"}]
            })
            
            # ç¬¬äºŒé˜¶æ®µï¼šè¶‹åŠ¿åˆ†æž
            trend_result = await self._analyze_trends(task, research_result)
            
            # æ›´æ–°æ‰§è¡Œæ—¥å¿—
            await self._update_execution_log(execution_log_id, {
                "execution_step": "summary_phase", 
                "progress_percentage": 80.0,
                "log_messages": [{"timestamp": datetime.now().isoformat(), "message": "Trend analysis completed, generating summary"}]
            })
            
            # ç¬¬ä¸‰é˜¶æ®µï¼šç”Ÿæˆæ‘˜è¦
            summary_result = await self._generate_summary(task, research_result, trend_result)
            
            # ç¬¬å››é˜¶æ®µï¼šä¿å­˜ç»“æžœ
            await self._save_results(task, research_result, trend_result, summary_result)
            
            # æ›´æ–°æ‰§è¡Œæ—¥å¿—
            await self._update_execution_log(execution_log_id, {
                "execution_step": "completed",
                "progress_percentage": 100.0,
                "log_messages": [{"timestamp": datetime.now().isoformat(), "message": "Task execution completed successfully"}]
            })
            
            # ç»„è£…æœ€ç»ˆç»“æžœ
            result.update({
                "success": True,
                "execution_time": time.time() - start_time,
                "summary": summary_result.get("summary", ""),
                "key_changes": summary_result.get("key_changes", []),
                "trend_score": trend_result.get("trend_score", 0.0),
                "sources_count": research_result.get("sources_count", 0),
                "raw_result": research_result.get("report", ""),
                "trend_data": trend_result,
                "log_messages": result["log_messages"]
            })
            
            logger.info(f"Task execution completed successfully: {task.topic}")
            
            # é€šè¿‡WebSocketå¹¿æ’­ç ”ç©¶ç»“æžœ
            if self.websocket_manager:
                await self._broadcast_result(task, result)
            
        except Exception as e:
            logger.error(f"Task execution failed: {e}")
            result["error"] = str(e)
            result["execution_time"] = time.time() - start_time
            
            # æ›´æ–°æ‰§è¡Œæ—¥å¿—
            await self._update_execution_log(execution_log_id, {
                "execution_step": "failed",
                "progress_percentage": 0.0,
                "log_messages": [{"timestamp": datetime.now().isoformat(), "message": f"Task execution failed: {str(e)}"}]
            })
        
        return result
    
    async def _conduct_research(self, task) -> Dict[str, Any]:
        """æ‰§è¡ŒGPTç ”ç©¶ - ä¼˜åŒ–ç‰ˆæœ¬"""
        try:
            logger.info(f"Conducting research for topic: {task.topic}")
            
            # æž„å»ºæŸ¥è¯¢å­—ç¬¦ä¸²
            query = self._build_research_query(task)
            
            # ä¼˜åŒ–çš„ç ”ç©¶å‚æ•°é…ç½®
            researcher_config = {
                "query": query,
                "report_type": self._get_report_type(task.report_type),
                "report_source": self._get_report_source(task.report_source),
                "tone": self._get_tone(task.tone),
                "max_subtopics": min(task.max_sources or 3, 5),  # é™åˆ¶å­ä¸»é¢˜æ•°é‡æå‡é€Ÿåº¦
                "verbose": False,  # å…³é—­è¯¦ç»†æ—¥å¿—æå‡æ€§èƒ½
                "headers": {"User-Agent": "GPT-Researcher-Scheduled/1.0"}
            }
            
            # å¦‚æžœæŒ‡å®šäº†æŸ¥è¯¢åŸŸå
            if task.query_domains:
                researcher_config["query_domains"] = task.query_domains[:5]  # é™åˆ¶åŸŸåæ•°é‡
            
            # åˆ›å»ºè‡ªå®šä¹‰é…ç½®æ¥ä¼˜åŒ–æ€§èƒ½
            custom_config = self._create_optimized_config(task)
            researcher_config["config_path"] = None  # ä½¿ç”¨é»˜è®¤é…ç½®ä½†ä¼šè¢«ä¸‹é¢çš„è®¾ç½®è¦†ç›–
            
            # åˆ›å»ºç ”ç©¶å™¨å®žä¾‹
            researcher = GPTResearcher(**researcher_config)
            
            # åº”ç”¨æ€§èƒ½ä¼˜åŒ–é…ç½®
            self._apply_performance_optimizations(researcher, custom_config)
            
            # æ‰§è¡Œç ”ç©¶ - ä½¿ç”¨è¶…æ—¶æŽ§åˆ¶
            start_time = time.time()
            try:
                # è®¾ç½®ç ”ç©¶è¶…æ—¶ï¼ˆæ ¹æ®åˆ†æžæ·±åº¦è°ƒæ•´ï¼‰
                timeout = self._get_research_timeout(task.analysis_depth)
                
                research_data = await asyncio.wait_for(
                    researcher.conduct_research(), 
                    timeout=timeout
                )
                report = await asyncio.wait_for(
                    researcher.write_report(), 
                    timeout=60  # æŠ¥å‘Šç”Ÿæˆæœ€å¤š1åˆ†é’Ÿ
                )
                
            except asyncio.TimeoutError:
                logger.warning(f"Research timeout for task {task.id}, using partial results")
                # å°è¯•èŽ·å–éƒ¨åˆ†ç»“æžœ
                report = await researcher.write_report() if hasattr(researcher, 'context') else "ç ”ç©¶è¶…æ—¶ï¼Œæ— æ³•ç”Ÿæˆå®Œæ•´æŠ¥å‘Š"
                research_data = getattr(researcher, 'context', [])
            
            # æå–ç ”ç©¶ç»“æžœä¿¡æ¯
            sources_count = len(getattr(researcher, 'visited_urls', set()))
            research_sources = getattr(researcher, 'research_sources', [])
            
            # æå–æ›´è¯¦ç»†çš„ç ”ç©¶ä¿¡æ¯
            research_costs = getattr(researcher, 'research_costs', 0.0)
            execution_time = time.time() - start_time
            
            return {
                "success": True,
                "report": report,
                "research_data": research_data,
                "research_sources": research_sources,
                "sources_count": sources_count,
                "query_used": query,
                "execution_time": execution_time,
                "research_costs": research_costs,
                "researcher_instance": researcher  # ä¿ç•™å®žä¾‹ä»¥ä¾¿åŽç»­åˆ†æž
            }
            
        except Exception as e:
            logger.error(f"Research failed: {e}")
            return {
                "success": False,
                "error": str(e)
            }
    
    def _get_report_type(self, report_type_str: str) -> str:
        """èŽ·å–æŠ¥å‘Šç±»åž‹"""
        type_mapping = {
            "research_report": ReportType.ResearchReport.value,
            "detailed_report": ReportType.DetailedReport.value,
            "multi_agents": "multi_agents",
            "deep": ReportType.DetailedReport.value
        }
        return type_mapping.get(report_type_str, ReportType.ResearchReport.value)
    
    def _get_report_source(self, source_str: str) -> str:
        """èŽ·å–æŠ¥å‘Šæ¥æº"""
        source_mapping = {
            "web": ReportSource.Web.value,
            "local": ReportSource.Local.value,
            "hybrid": ReportSource.Hybrid.value
        }
        return source_mapping.get(source_str, ReportSource.Web.value)
    
    def _get_tone(self, tone_str: str) -> Tone:
        """èŽ·å–è¯­è°ƒ"""
        tone_mapping = {
            "objective": Tone.Objective,
            "formal": Tone.Formal,
            "analytical": Tone.Analytical,
            "persuasive": Tone.Persuasive,
            "informative": Tone.Informative,
            "explanatory": Tone.Explanatory,
            "descriptive": Tone.Descriptive,
            "critical": Tone.Critical,
            "comparative": Tone.Comparative,
            "speculative": Tone.Speculative,
            "reflective": Tone.Reflective,
            "narrative": Tone.Narrative,
            "humorous": Tone.Humorous,
            "optimistic": Tone.Optimistic,
            "pessimistic": Tone.Pessimistic,
            "simple": Tone.Simple,
            "casual": Tone.Casual
        }
        return tone_mapping.get(tone_str, Tone.Objective)
    
    def _get_research_timeout(self, analysis_depth: str) -> int:
        """æ ¹æ®åˆ†æžæ·±åº¦èŽ·å–ç ”ç©¶è¶…æ—¶æ—¶é—´"""
        timeout_mapping = {
            "basic": 120,      # 2åˆ†é’Ÿ
            "detailed": 300,   # 5åˆ†é’Ÿ
            "deep": 600        # 10åˆ†é’Ÿ
        }
        return timeout_mapping.get(analysis_depth, 180)  # é»˜è®¤3åˆ†é’Ÿ
    
    def _create_optimized_config(self, task) -> Dict[str, Any]:
        """åˆ›å»ºä¼˜åŒ–çš„é…ç½®"""
        return ScheduledResearchConfig.get_optimized_config(task)
    
    def _apply_performance_optimizations(self, researcher, custom_config):
        """åº”ç”¨æ€§èƒ½ä¼˜åŒ–é…ç½®åˆ°ç ”ç©¶å™¨"""
        try:
            # æ›´æ–°é…ç½®
            for key, value in custom_config.items():
                if hasattr(researcher.cfg, key.lower()):
                    setattr(researcher.cfg, key.lower(), value)
            
            # ä¼˜åŒ–å†…å­˜ä½¿ç”¨
            researcher.cfg.browse_chunk_max_length = 4096  # å‡å°‘chunkå¤§å°
            researcher.cfg.summary_token_limit = 500      # å‡å°‘æ‘˜è¦token
            
        except Exception as e:
            logger.warning(f"Failed to apply some optimizations: {e}")
    
    def _build_research_query(self, task) -> str:
        """æž„å»ºç ”ç©¶æŸ¥è¯¢å­—ç¬¦ä¸²ï¼ˆåŒ…å«å†…å®¹å®‰å…¨å¤„ç†ï¼‰"""
        try:
            # ç”ŸæˆåŸºç¡€æŸ¥è¯¢
            base_query = ScheduledResearchPrompts.generate_trend_research_query(task)
            
            # è¿›è¡Œå†…å®¹å®‰å…¨é¢„å¤„ç†
            processed_query = ContentSafetyChecker.preprocess_query_for_safety(base_query)
            
            # æ£€æŸ¥å¤„ç†åŽçš„æŸ¥è¯¢å®‰å…¨æ€§
            is_safe, risky_words, suggestions = ContentSafetyChecker.check_query_safety(processed_query)
            
            if not is_safe and risky_words:
                logger.warning(f"Detected potentially sensitive content in query for task {task.id}: {risky_words}")
                
                # å¦‚æžœä»æœ‰æ•æ„Ÿè¯ï¼Œä½¿ç”¨å»ºè®®çš„å®‰å…¨æŸ¥è¯¢
                if suggestions:
                    safe_query = ContentSafetyChecker.suggest_safe_query(processed_query)
                    logger.info(f"Using safe alternative query for task {task.id}: {safe_query}")
                    return safe_query
            
            return processed_query
            
        except Exception as e:
            logger.error(f"Error in query safety processing: {e}")
            # å¦‚æžœå®‰å…¨å¤„ç†å¤±è´¥ï¼Œå›žé€€åˆ°åŸºç¡€æŸ¥è¯¢
            return ScheduledResearchPrompts.generate_trend_research_query(task)
    
    async def _analyze_trends(self, task, research_result) -> Dict[str, Any]:
        """åˆ†æžè¶‹åŠ¿å˜åŒ–"""
        try:
            logger.info(f"Analyzing trends for task: {task.topic}")
            
            # èŽ·å–åŽ†å²æ•°æ®è¿›è¡Œå¯¹æ¯”
            historical_data = await self._get_historical_data(task.id)
            
            # ä½¿ç”¨è¶‹åŠ¿åˆ†æžå™¨
            trend_analysis = await self.trend_analyzer.analyze_trends(
                task=task,
                current_result=research_result,
                historical_data=historical_data
            )
            
            return trend_analysis
            
        except Exception as e:
            logger.error(f"Trend analysis failed: {e}")
            return {
                "trend_score": 5.0,  # é»˜è®¤ä¸­ç­‰è¶‹åŠ¿åˆ†æ•°
                "keyword_trends": {},
                "sentiment_changes": {},
                "new_topics": [],
                "error": str(e)
            }
    
    async def _generate_summary(self, task, research_result, trend_result) -> Dict[str, Any]:
        """ç”ŸæˆåŠ¨æ€æ‘˜è¦"""
        try:
            logger.info(f"Generating summary for task: {task.topic}")
            
            # ä½¿ç”¨æ‘˜è¦ç”Ÿæˆå™¨
            summary_data = await self.summary_generator.generate_dynamic_summary(
                task=task,
                research_result=research_result,
                trend_result=trend_result
            )
            
            return summary_data
            
        except Exception as e:
            logger.error(f"Summary generation failed: {e}")
            return {
                "summary": research_result.get("report", "")[:500] + "...",  # æˆªå–å‰500å­—ç¬¦ä½œä¸ºæ‘˜è¦
                "key_changes": [],
                "key_findings": [],
                "error": str(e)
            }
    
    async def _get_historical_data(self, task_id: str) -> List[Dict]:
        """èŽ·å–åŽ†å²æ•°æ®ç”¨äºŽè¶‹åŠ¿å¯¹æ¯”"""
        try:
            # èŽ·å–æœ€è¿‘çš„æˆåŠŸè®°å½•
            histories = ResearchHistoryDAO.get_successful_histories(task_id, limit=5)
            
            historical_data = []
            for history in histories:
                historical_data.append({
                    "executed_at": history.executed_at,
                    "summary": history.summary,
                    "key_findings": history.key_findings,
                    "trend_score": history.trend_score,
                    "sentiment_score": history.sentiment_score
                })
            
            return historical_data
            
        except Exception as e:
            logger.error(f"Failed to get historical data: {e}")
            return []
    
    async def _save_results(self, task, research_result, trend_result, summary_result):
        """ä¿å­˜æ‰§è¡Œç»“æžœåˆ°æ•°æ®åº“"""
        try:
            # ä¿å­˜ç ”ç©¶åŽ†å²è®°å½•
            history_data = {
                "task_id": task.id,
                "raw_result": research_result.get("report", ""),
                "summary": summary_result.get("summary", ""),
                "key_findings": summary_result.get("key_findings", []),
                "key_changes": summary_result.get("key_changes", []),
                "sources_count": research_result.get("sources_count", 0),
                "trend_score": trend_result.get("trend_score", 0.0),
                "sentiment_score": trend_result.get("sentiment_score", 0.0),
                "status": "success",
                "research_config": {
                    "query": research_result.get("query_used", ""),
                    "analysis_depth": task.analysis_depth,
                    "source_types": task.source_types
                }
            }
            
            history = ResearchHistoryDAO.create_history(history_data)
            logger.info(f"Saved research history: {history.id}")
            
            # ä¿å­˜è¶‹åŠ¿æ•°æ®
            if trend_result and not trend_result.get("error"):
                trend_data = {
                    "task_id": task.id,
                    "period_start": datetime.now() - timedelta(hours=task.interval_hours),
                    "period_end": datetime.now(),
                    "keyword_trends": trend_result.get("keyword_trends", {}),
                    "sentiment_changes": trend_result.get("sentiment_changes", {}),
                    "new_topics": trend_result.get("new_topics", []),
                    "emerging_keywords": trend_result.get("emerging_keywords", []),
                    "activity_level": trend_result.get("activity_level", 0.0),
                    "change_magnitude": trend_result.get("change_magnitude", 0.0),
                    "confidence_score": trend_result.get("confidence_score", 0.0),
                    "comparison_data": trend_result.get("comparison_data", {}),
                    "anomaly_detected": trend_result.get("anomaly_detected", False),
                    "anomaly_description": trend_result.get("anomaly_description", "")
                }
                
                trend = TrendDataDAO.create_trend_data(trend_data)
                logger.info(f"Saved trend data: {trend.id}")
            
        except Exception as e:
            logger.error(f"Failed to save results: {e}")
            # è¿™é‡Œä¸æŠ›å‡ºå¼‚å¸¸ï¼Œå› ä¸ºç ”ç©¶å·²ç»å®Œæˆï¼Œåªæ˜¯ä¿å­˜å¤±è´¥
    
    async def _update_execution_log(self, log_id: Optional[str], update_data: Dict):
        """æ›´æ–°æ‰§è¡Œæ—¥å¿—"""
        if not log_id:
            return
        
        try:
            TaskExecutionLogDAO.update_log(log_id, update_data)
        except Exception as e:
            logger.error(f"Failed to update execution log: {e}")
    
    async def test_research_configuration(self, task_data: Dict) -> Dict[str, Any]:
        """æµ‹è¯•ç ”ç©¶é…ç½®ï¼ˆä¸ä¿å­˜ç»“æžœï¼‰"""
        try:
            logger.info("Testing research configuration...")
            
            # åˆ›å»ºä¸´æ—¶ä»»åŠ¡å¯¹è±¡
            from types import SimpleNamespace
            temp_task = SimpleNamespace(**task_data)
            
            # æž„å»ºæŸ¥è¯¢
            query = self._build_research_query(temp_task)
            
            # æ‰§è¡Œç®€åŒ–çš„ç ”ç©¶ï¼ˆé™åˆ¶æ—¶é—´å’Œèµ„æºï¼‰
            researcher_config = {
                "query": query,
                "report_type": "research_report",
                "report_source": "web",
                "tone": Tone.Objective,
                "max_subtopics": 3,  # é™åˆ¶å­è¯é¢˜æ•°é‡
                "verbose": False
            }
            
            researcher = GPTResearcher(**researcher_config)
            
            # åªè¿›è¡Œåˆæ­¥ç ”ç©¶ï¼Œä¸ç”Ÿæˆå®Œæ•´æŠ¥å‘Š
            research_data = await researcher.conduct_research()
            
            return {
                "success": True,
                "query_generated": query,
                "sources_found": len(getattr(researcher, 'visited_urls', set())),
                "research_preview": str(research_data)[:200] + "..."
            }
            
        except Exception as e:
            logger.error(f"Research configuration test failed: {e}")
            return {
                "success": False,
                "error": str(e)
            }
    
    async def _broadcast_result(self, task, result):
        """é€šè¿‡WebSocketå¹¿æ’­ç ”ç©¶ç»“æžœ"""
        try:
            # å‡†å¤‡å¹¿æ’­æ•°æ®
            broadcast_data = {
                "task_id": task.id,
                "topic": task.topic,
                "timestamp": datetime.now().isoformat(),
                "summary": result.get("summary", ""),
                "key_changes": result.get("key_changes", []),
                "trend_score": result.get("trend_score", 0.0),
                "sources_count": result.get("sources_count", 0),
                "status": "completed" if result.get("success") else "failed",
                "execution_time": result.get("execution_time", 0.0)
            }
            
            # å¹¿æ’­ç»“æžœ
            await self.websocket_manager.broadcast_scheduled_result(broadcast_data)
            
            # å¦‚æžœè¶‹åŠ¿åˆ†æ•°è¶…è¿‡é€šçŸ¥é˜ˆå€¼ï¼Œå‘é€é€šçŸ¥
            if (result.get("trend_score", 0) >= task.notification_threshold and 
                task.enable_notifications):
                
                notification_data = {
                    "title": f"ðŸ”¥ é‡è¦è¶‹åŠ¿å˜åŒ–æ£€æµ‹",
                    "message": f"è¯é¢˜ '{task.topic}' å‡ºçŽ°é‡è¦å˜åŒ–ï¼Œè¶‹åŠ¿åˆ†æ•°: {result.get('trend_score', 0):.1f}",
                    "level": "warning" if result.get("trend_score", 0) >= 8 else "info",
                    "task_id": task.id,
                    "timestamp": datetime.now().isoformat()
                }
                
                await self.websocket_manager.send_scheduled_notification(notification_data)
            
            logger.info(f"Successfully broadcasted result for task: {task.id}")
            
        except Exception as e:
            logger.error(f"Failed to broadcast result: {e}")
