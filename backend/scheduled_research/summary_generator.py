"""
åŠ¨æ€æ‘˜è¦ç”Ÿæˆå™¨ - ç”ŸæˆAIé©±åŠ¨çš„æ™ºèƒ½æ‘˜è¦
Dynamic Summary Generator - Generates AI-driven intelligent summaries
"""
import logging
import json
from datetime import datetime
from typing import Dict, List, Any, Optional

from .prompts import ScheduledResearchPrompts
import asyncio

logger = logging.getLogger(__name__)


class DynamicSummaryGenerator:
    """
    åŠ¨æ€æ‘˜è¦ç”Ÿæˆå™¨
    Generates dynamic summaries based on research results and trend analysis
    """
    
    def __init__(self):
        """åˆå§‹åŒ–æ‘˜è¦ç”Ÿæˆå™¨"""
        self.summary_templates = {
            "trending_up": "ğŸ“ˆ {topic} å‘ˆç°ä¸Šå‡è¶‹åŠ¿ï¼Œæ´»è·ƒåº¦æ˜¾è‘—æå‡",
            "trending_down": "ğŸ“‰ {topic} æ´»è·ƒåº¦æœ‰æ‰€ä¸‹é™ï¼Œéœ€è¦å…³æ³¨å‘å±•åŠ¨å‘", 
            "stable": "ğŸ“Š {topic} ä¿æŒç¨³å®šå‘å±•ï¼Œæ— æ˜æ˜¾æ³¢åŠ¨",
            "emerging": "ğŸš€ {topic} å‡ºç°æ–°çš„å‘å±•æ–¹å‘ï¼Œå€¼å¾—é‡ç‚¹å…³æ³¨",
            "anomaly": "âš ï¸ {topic} æ£€æµ‹åˆ°å¼‚å¸¸å˜åŒ–ï¼Œå»ºè®®æ·±å…¥åˆ†æ"
        }
    
    async def generate_dynamic_summary(self, task, research_result: Dict[str, Any], trend_result: Dict[str, Any]) -> Dict[str, Any]:
        """
        ç”ŸæˆåŠ¨æ€æ‘˜è¦
        
        Args:
            task: ä»»åŠ¡å¯¹è±¡
            research_result: ç ”ç©¶ç»“æœ
            trend_result: è¶‹åŠ¿åˆ†æç»“æœ
            
        Returns:
            DictåŒ…å«ç”Ÿæˆçš„æ‘˜è¦å’Œç›¸å…³ä¿¡æ¯
        """
        try:
            logger.info(f"Generating dynamic summary for: {task.topic}")
            
            # ç®€åŒ–åˆ†ç±»ï¼Œå¢åŠ å…·ä½“å†…å®¹
            summary_data = {
                "summary": "",
                "key_findings": [],
                "key_changes": []
            }
            
            # ç”Ÿæˆç»¼åˆæ€§è¯¦ç»†æ‘˜è¦ï¼ˆåŒ…å«è¶‹åŠ¿ã€åˆ†æã€å»ºè®®ç­‰æ‰€æœ‰å†…å®¹ï¼‰
            summary_data["summary"] = await self._generate_comprehensive_summary(task, research_result, trend_result)
            
            # ç”Ÿæˆè¯¦ç»†çš„å…³é”®å‘ç°ï¼ˆåŒ…å«å…·ä½“å†…å®¹å’Œåˆ†æï¼‰
            summary_data["key_findings"] = self._extract_detailed_findings(research_result, trend_result)
            
            # ç”Ÿæˆè¯¦ç»†çš„å˜åŒ–åˆ†æï¼ˆåŒ…å«è¶‹åŠ¿å˜åŒ–å’Œå…·ä½“æ•°æ®ï¼‰
            summary_data["key_changes"] = self._identify_detailed_changes(trend_result)
            
            logger.info(f"Dynamic summary generated successfully")
            
            return summary_data
            
        except Exception as e:
            logger.error(f"Dynamic summary generation failed: {e}")
            return self._create_fallback_summary(research_result, task)
    
    async def _generate_comprehensive_summary(self, task, research_result: Dict[str, Any], trend_result: Dict[str, Any]) -> str:
        """ç”Ÿæˆç»¼åˆæ€§è¯¦ç»†æ‘˜è¦"""
        try:
            # è·å–è¶‹åŠ¿åˆ†æ•°å’ŒçŠ¶æ€
            trend_score = trend_result.get("trend_score", 5.0)
            activity_level = trend_result.get("activity_level", 5.0)
            anomaly_detected = trend_result.get("anomaly_detected", False)
            
            # ç¡®å®šè¶‹åŠ¿çŠ¶æ€
            if anomaly_detected:
                trend_status = "anomaly"
            elif trend_score > 7.5:
                trend_status = "trending_up"
            elif trend_score < 3.5:
                trend_status = "trending_down"
            elif len(trend_result.get("new_topics", [])) > 3:
                trend_status = "emerging"
            else:
                trend_status = "stable"
            
            # åŸºç¡€æ‘˜è¦æ¨¡æ¿
            base_summary = self.summary_templates.get(trend_status, "").format(topic=task.topic)
            
            # æ·»åŠ è¯¦ç»†ä¿¡æ¯
            details = []
            
            # æ·»åŠ æ´»è·ƒåº¦ä¿¡æ¯
            if activity_level > 7.0:
                details.append(f"å½“å‰æ´»è·ƒåº¦è¾ƒé«˜ (è¯„åˆ†: {activity_level:.1f}/10)")
            elif activity_level < 4.0:
                details.append(f"å½“å‰æ´»è·ƒåº¦è¾ƒä½ (è¯„åˆ†: {activity_level:.1f}/10)")
            
            # æ·»åŠ æ–°å‘ç°ä¿¡æ¯
            new_topics = trend_result.get("new_topics", [])
            if new_topics:
                details.append(f"å‘ç° {len(new_topics)} ä¸ªæ–°ç›¸å…³è¯é¢˜")
            
            # æ·»åŠ å…³é”®è¯è¶‹åŠ¿
            keyword_trends = trend_result.get("keyword_trends", {})
            trending_keywords = [k for k, v in keyword_trends.items() if v > 7.0]
            if trending_keywords:
                details.append(f"çƒ­é—¨å…³é”®è¯: {', '.join(trending_keywords[:3])}")
            
            # æ·»åŠ æƒ…æ„Ÿå˜åŒ–
            sentiment_changes = trend_result.get("sentiment_changes", {})
            for sentiment_type, data in sentiment_changes.items():
                if isinstance(data, dict) and data.get("trend") in ["up", "down"]:
                    if sentiment_type == "positive" and data.get("trend") == "up":
                        details.append("æ­£é¢æƒ…ç»ªå‘ˆä¸Šå‡è¶‹åŠ¿")
                    elif sentiment_type == "negative" and data.get("trend") == "up":
                        details.append("è´Ÿé¢æƒ…ç»ªæœ‰æ‰€å¢åŠ ")
            
            # æ„å»ºè¯¦ç»†çš„ç»¼åˆæ‘˜è¦
            summary_parts = [base_summary]
            
            # æ·»åŠ è¯¦ç»†çš„æ•°æ®åˆ†æ
            if details:
                summary_parts.append(f"è¯¦ç»†åˆ†æï¼š{' '.join(details)}ã€‚")
            
            # æ·»åŠ å…·ä½“çš„ç ”ç©¶å†…å®¹æ‘˜å½•
            report_content = research_result.get("report", "")
            if report_content and len(report_content) > 100:
                # æå–æŠ¥å‘Šçš„å…³é”®æ®µè½
                key_paragraphs = self._extract_key_paragraphs(report_content)
                if key_paragraphs:
                    summary_parts.append(f"\n\n**ä¸»è¦å‘ç°ï¼š**\n{key_paragraphs}")
            
            # æ·»åŠ è¶‹åŠ¿åˆ†æè¯¦æƒ…
            trend_details = self._generate_detailed_trend_analysis(trend_result)
            if trend_details:
                summary_parts.append(f"\n\n**è¶‹åŠ¿åˆ†æï¼š**\n{trend_details}")
            
            # æ·»åŠ è¡ŒåŠ¨å»ºè®®å’Œåç»­å…³æ³¨é‡ç‚¹
            recommendations = self._generate_comprehensive_recommendations(task, trend_result)
            if recommendations:
                summary_parts.append(f"\n\n**å»ºè®®ä¸åç»­å…³æ³¨ï¼š**\n{recommendations}")
            
            # æ·»åŠ æ—¶é—´æˆ³
            timestamp = datetime.now().strftime("%Yå¹´%mæœˆ%dæ—¥ %H:%M")
            summary_parts.append(f"\n\n*æ•°æ®æ›´æ–°æ—¶é—´: {timestamp}*")
            
            return '\n'.join(summary_parts)
            
        except Exception as e:
            logger.error(f"Main summary generation failed: {e}")
            return f"å…³äº {task.topic} çš„æœ€æ–°åˆ†ææŠ¥å‘Šå·²ç”Ÿæˆã€‚æ•°æ®æ›´æ–°æ—¶é—´: {datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥ %H:%M')}"
    
    def _extract_key_paragraphs(self, report_content: str) -> str:
        """ä»æŠ¥å‘Šä¸­æå–å…³é”®æ®µè½"""
        try:
            # åˆ†å‰²æˆæ®µè½
            paragraphs = [p.strip() for p in report_content.split('\n\n') if p.strip()]
            if not paragraphs:
                paragraphs = [p.strip() for p in report_content.split('\n') if p.strip() and len(p.strip()) > 50]
            
            # é€‰æ‹©æœ€æœ‰ä»·å€¼çš„æ®µè½ï¼ˆé•¿åº¦é€‚ä¸­ï¼ŒåŒ…å«å…³é”®è¯ï¼‰
            key_paragraphs = []
            important_keywords = ["çªç ´", "åˆ›æ–°", "å‘å±•", "å¢é•¿", "å˜åŒ–", "è¶‹åŠ¿", "å½±å“", "é‡è¦", "å…³é”®", "æ˜¾è‘—", "ä¸»è¦", "æ ¸å¿ƒ", "æ–°å…´"]
            
            for paragraph in paragraphs[:10]:  # æ£€æŸ¥å‰10ä¸ªæ®µè½
                if 50 <= len(paragraph) <= 300:  # é•¿åº¦é€‚ä¸­
                    keyword_count = sum(1 for keyword in important_keywords if keyword in paragraph)
                    if keyword_count >= 1:  # åŒ…å«è‡³å°‘ä¸€ä¸ªå…³é”®è¯
                        key_paragraphs.append(paragraph)
                        if len(key_paragraphs) >= 3:  # æœ€å¤šé€‰æ‹©3ä¸ªæ®µè½
                            break
            
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°åˆé€‚çš„æ®µè½ï¼Œé€‰æ‹©å‰å‡ ä¸ªè¾ƒé•¿çš„æ®µè½
            if not key_paragraphs:
                key_paragraphs = [p for p in paragraphs[:3] if len(p) > 30]
            
            return '\n\n'.join(key_paragraphs[:3])
            
        except Exception as e:
            logger.error(f"Key paragraphs extraction failed: {e}")
            return ""

    def _generate_detailed_trend_analysis(self, trend_result: Dict[str, Any]) -> str:
        """ç”Ÿæˆè¯¦ç»†çš„è¶‹åŠ¿åˆ†æ"""
        try:
            analysis_parts = []
            
            # åŸºç¡€æŒ‡æ ‡åˆ†æ
            trend_score = trend_result.get("trend_score", 5.0)
            activity_level = trend_result.get("activity_level", 5.0)
            change_magnitude = trend_result.get("change_magnitude", 0.0)
            confidence_score = trend_result.get("confidence_score", 0.5)
            
            # è¶‹åŠ¿åˆ†æ
            if trend_score > 7.5:
                analysis_parts.append(f"**è¶‹åŠ¿å¼ºåº¦ï¼š** å½“å‰å‘ˆç°å¼ºåŠ²ä¸Šå‡è¶‹åŠ¿ (è¯„åˆ†: {trend_score:.1f}/10)ï¼Œè¡¨æ˜è¯¥è¯é¢˜æ­£è·å¾—è¶Šæ¥è¶Šå¤šçš„å…³æ³¨å’Œè®¨è®ºã€‚")
            elif trend_score > 6.0:
                analysis_parts.append(f"**è¶‹åŠ¿å¼ºåº¦ï¼š** è¡¨ç°å‡ºæ¸©å’Œçš„ä¸Šå‡è¶‹åŠ¿ (è¯„åˆ†: {trend_score:.1f}/10)ï¼Œç›¸å…³æ´»åŠ¨å’Œè®¨è®ºæœ‰æ‰€å¢åŠ ã€‚")
            elif trend_score < 3.5:
                analysis_parts.append(f"**è¶‹åŠ¿å¼ºåº¦ï¼š** å‡ºç°ä¸‹é™è¶‹åŠ¿ (è¯„åˆ†: {trend_score:.1f}/10)ï¼Œå…³æ³¨åº¦å’Œæ´»è·ƒç¨‹åº¦æœ‰æ‰€å‡å°‘ã€‚")
            else:
                analysis_parts.append(f"**è¶‹åŠ¿å¼ºåº¦ï¼š** ä¿æŒç›¸å¯¹ç¨³å®šçš„å‘å±•æ€åŠ¿ (è¯„åˆ†: {trend_score:.1f}/10)ï¼Œæ²¡æœ‰æ˜æ˜¾çš„æ³¢åŠ¨å˜åŒ–ã€‚")
            
            # æ´»è·ƒåº¦åˆ†æ
            if activity_level > 7.0:
                analysis_parts.append(f"**æ´»è·ƒç¨‹åº¦ï¼š** å½“å‰å¤„äºé«˜åº¦æ´»è·ƒçŠ¶æ€ (è¯„åˆ†: {activity_level:.1f}/10)ï¼Œä¿¡æ¯æ›´æ–°é¢‘ç¹ï¼Œè®¨è®ºçƒ­çƒˆã€‚")
            elif activity_level > 5.0:
                analysis_parts.append(f"**æ´»è·ƒç¨‹åº¦ï¼š** ç»´æŒä¸­ç­‰æ´»è·ƒæ°´å¹³ (è¯„åˆ†: {activity_level:.1f}/10)ï¼Œæœ‰ç¨³å®šçš„ä¿¡æ¯äº§å‡ºå’Œå…³æ³¨ã€‚")
            else:
                analysis_parts.append(f"**æ´»è·ƒç¨‹åº¦ï¼š** æ´»è·ƒåº¦ç›¸å¯¹è¾ƒä½ (è¯„åˆ†: {activity_level:.1f}/10)ï¼Œå¯èƒ½éœ€è¦å¯»æ‰¾æ–°çš„ä¿¡æ¯æºæˆ–è°ƒæ•´å…³é”®è¯ã€‚")
            
            # å˜åŒ–ç¨‹åº¦åˆ†æ
            if change_magnitude > 6.0:
                analysis_parts.append(f"**å˜åŒ–ç¨‹åº¦ï¼š** å‘ç”Ÿäº†æ˜¾è‘—å˜åŒ– (è¯„åˆ†: {change_magnitude:.1f}/10)ï¼Œå‡ºç°äº†å€¼å¾—å…³æ³¨çš„æ–°åŠ¨å‘æˆ–è½¬æŠ˜ç‚¹ã€‚")
            elif change_magnitude > 3.0:
                analysis_parts.append(f"**å˜åŒ–ç¨‹åº¦ï¼š** å‡ºç°äº†ä¸€å®šç¨‹åº¦çš„å˜åŒ– (è¯„åˆ†: {change_magnitude:.1f}/10)ï¼Œè¶‹åŠ¿æ­£åœ¨é€æ­¥æ¼”è¿›ã€‚")
            else:
                analysis_parts.append(f"**å˜åŒ–ç¨‹åº¦ï¼š** å˜åŒ–ç›¸å¯¹å¹³ç¼“ (è¯„åˆ†: {change_magnitude:.1f}/10)ï¼Œä¿æŒäº†ç›¸å¯¹ç¨³å®šçš„å‘å±•æ¨¡å¼ã€‚")
            
            # å…³é”®è¯è¶‹åŠ¿è¯¦æƒ…
            keyword_trends = trend_result.get("keyword_trends", {})
            if keyword_trends:
                trending_up = [(k, v) for k, v in keyword_trends.items() if v > 7.0]
                trending_down = [(k, v) for k, v in keyword_trends.items() if v < 4.0]
                
                if trending_up:
                    trending_keywords = ', '.join([f"{k}({v:.1f})" for k, v in trending_up[:3]])
                    analysis_parts.append(f"**çƒ­é—¨å…³é”®è¯ï¼š** {trending_keywords} - è¿™äº›å…³é”®è¯æ˜¾ç¤ºå‡ºå¼ºåŠ²çš„ä¸Šå‡è¶‹åŠ¿ï¼Œå€¼å¾—é‡ç‚¹å…³æ³¨ã€‚")
                
                if trending_down:
                    declining_keywords = ', '.join([f"{k}({v:.1f})" for k, v in trending_down[:3]])
                    analysis_parts.append(f"**çƒ­åº¦ä¸‹é™ï¼š** {declining_keywords} - è¿™äº›å…³é”®è¯çš„å…³æ³¨åº¦æœ‰æ‰€ä¸‹é™ï¼Œå¯èƒ½åæ˜ äº†è¯é¢˜é‡ç‚¹çš„è½¬ç§»ã€‚")
            
            # æ–°å…´å†…å®¹åˆ†æ
            new_topics = trend_result.get("new_topics", [])
            emerging_keywords = trend_result.get("emerging_keywords", [])
            
            if new_topics or emerging_keywords:
                emerging_content = []
                if new_topics:
                    emerging_content.append(f"æ–°è¯é¢˜: {', '.join(new_topics[:3])}")
                if emerging_keywords:
                    emerging_content.append(f"æ–°å…³é”®è¯: {', '.join(emerging_keywords[:3])}")
                
                analysis_parts.append(f"**æ–°å…´å†…å®¹ï¼š** {'; '.join(emerging_content)} - è¿™äº›æ–°å‡ºç°çš„å…ƒç´ å¯èƒ½ä»£è¡¨äº†è¯¥é¢†åŸŸçš„æœ€æ–°å‘å±•æ–¹å‘ã€‚")
            
            # æ•°æ®å¯é æ€§
            if confidence_score > 0.8:
                analysis_parts.append(f"**æ•°æ®å¯é æ€§ï¼š** é«˜ç½®ä¿¡åº¦ ({confidence_score:.1%}) - åŸºäºå……è¶³çš„å†å²æ•°æ®å’Œé«˜è´¨é‡çš„ä¿¡æ¯æºã€‚")
            elif confidence_score > 0.5:
                analysis_parts.append(f"**æ•°æ®å¯é æ€§ï¼š** ä¸­ç­‰ç½®ä¿¡åº¦ ({confidence_score:.1%}) - æœ‰ä¸€å®šçš„å†å²æ•°æ®æ”¯æ’‘ï¼Œç»“è®ºè¾ƒä¸ºå¯é ã€‚")
            else:
                analysis_parts.append(f"**æ•°æ®å¯é æ€§ï¼š** ä½ç½®ä¿¡åº¦ ({confidence_score:.1%}) - å†å²æ•°æ®æœ‰é™ï¼Œå»ºè®®ç§¯ç´¯æ›´å¤šæ•°æ®åå†åšåˆ¤æ–­ã€‚")
            
            return '\n\n'.join(analysis_parts)
            
        except Exception as e:
            logger.error(f"Detailed trend analysis generation failed: {e}")
            return "è¶‹åŠ¿åˆ†ææ­£åœ¨è¿›è¡Œä¸­ï¼Œè¯·ç¨åæŸ¥çœ‹è¯¦ç»†ç»“æœã€‚"

    def _generate_comprehensive_recommendations(self, task, trend_result: Dict[str, Any]) -> str:
        """ç”Ÿæˆç»¼åˆæ€§å»ºè®®å’Œåç»­å…³æ³¨é‡ç‚¹"""
        try:
            recommendations = []
            
            trend_score = trend_result.get("trend_score", 5.0)
            activity_level = trend_result.get("activity_level", 5.0)
            new_topics = trend_result.get("new_topics", [])
            emerging_keywords = trend_result.get("emerging_keywords", [])
            anomaly_detected = trend_result.get("anomaly_detected", False)
            
            # åŸºäºè¶‹åŠ¿çš„å»ºè®®
            if trend_score > 8.0:
                recommendations.append("**ç«‹å³è¡ŒåŠ¨å»ºè®®ï¼š** å½“å‰è¶‹åŠ¿éå¸¸å¼ºåŠ²ï¼Œå»ºè®®å¯†åˆ‡å…³æ³¨æœ€æ–°å‘å±•ï¼Œè€ƒè™‘åŠ å¤§æŠ•å…¥æˆ–æé«˜å…³æ³¨ä¼˜å…ˆçº§ã€‚è¿™å¯èƒ½æ˜¯ä¸€ä¸ªé‡è¦çš„å‘å±•çª—å£æœŸã€‚")
            elif trend_score > 6.0:
                recommendations.append("**æŒç»­å…³æ³¨å»ºè®®ï¼š** è¶‹åŠ¿å‘å¥½ï¼Œå»ºè®®ä¿æŒå½“å‰çš„ç›‘æµ‹é¢‘ç‡ï¼ŒåŠæ—¶è·Ÿè¿›ç›¸å…³åŠ¨æ€ï¼Œé€‚æ—¶è°ƒæ•´ç­–ç•¥æ–¹å‘ã€‚")
            elif trend_score < 3.0:
                recommendations.append("**ç­–ç•¥è°ƒæ•´å»ºè®®ï¼š** å½“å‰è¶‹åŠ¿èµ°ä½ï¼Œå»ºè®®è¯„ä¼°æ˜¯å¦éœ€è¦è°ƒæ•´ç ”ç©¶æ–¹å‘æˆ–é‡æ–°å®šä½å…³æ³¨é‡ç‚¹ï¼Œå¯èƒ½éœ€è¦å¯»æ‰¾æ–°çš„åˆ‡å…¥è§’åº¦ã€‚")
            else:
                recommendations.append("**ç¨³å®šç›‘æµ‹å»ºè®®ï¼š** è¶‹åŠ¿ç›¸å¯¹ç¨³å®šï¼Œå»ºè®®ç»§ç»­å®šæœŸç›‘æµ‹ï¼Œå…³æ³¨æ½œåœ¨çš„å˜åŒ–ä¿¡å·å’Œæ–°å…´å‘å±•æ–¹å‘ã€‚")
            
            # åŸºäºæ´»è·ƒåº¦çš„å»ºè®®
            if activity_level > 8.0:
                recommendations.append("**å®æ—¶è·Ÿè¿›ï¼š** å½“å‰æ´»è·ƒåº¦æé«˜ï¼Œä¿¡æ¯æ›´æ–°é¢‘ç¹ï¼Œå»ºè®®å¯ç”¨å®æ—¶ç›‘æµ‹æ¨¡å¼ï¼Œç¡®ä¿ä¸é”™è¿‡é‡è¦ä¿¡æ¯å’Œå‘å±•åŠ¨å‘ã€‚")
            elif activity_level < 3.0:
                recommendations.append("**ä¿¡æ¯æºä¼˜åŒ–ï¼š** æ´»è·ƒåº¦è¾ƒä½ï¼Œå»ºè®®æ‰©å±•ä¿¡æ¯æºèŒƒå›´ï¼Œè°ƒæ•´æœç´¢å…³é”®è¯ç»„åˆï¼Œæˆ–ç¼©çŸ­ç›‘æµ‹é—´éš”ä»¥æ•è·æ›´å¤šåŠ¨æ€ã€‚")
            
            # åŸºäºæ–°å†…å®¹çš„å»ºè®®
            if len(new_topics) > 2 or len(emerging_keywords) > 3:
                focus_areas = []
                if new_topics:
                    focus_areas.extend(new_topics[:3])
                if emerging_keywords:
                    focus_areas.extend(emerging_keywords[:3])
                
                unique_areas = list(dict.fromkeys(focus_areas))[:4]
                recommendations.append(f"**æ–°å…´é¢†åŸŸå…³æ³¨ï¼š** å‘ç°äº†å¤šä¸ªæ–°çš„å‘å±•æ–¹å‘ï¼Œå»ºè®®é‡ç‚¹å…³æ³¨ä»¥ä¸‹é¢†åŸŸï¼š{', '.join(unique_areas)}ã€‚è¿™äº›å¯èƒ½ä»£è¡¨äº†æœªæ¥çš„é‡è¦å‘å±•è¶‹åŠ¿ã€‚")
            
            # åŸºäºå¼‚å¸¸çš„å»ºè®®
            if anomaly_detected:
                anomaly_desc = trend_result.get("anomaly_description", "")
                recommendations.append(f"**å¼‚å¸¸æƒ…å†µå¤„ç†ï¼š** æ£€æµ‹åˆ°å¼‚å¸¸å˜åŒ–æ¨¡å¼ - {anomaly_desc}ã€‚å»ºè®®ç«‹å³è¿›è¡Œæ·±åº¦åˆ†æï¼Œäº†è§£å˜åŒ–åŸå› ï¼Œè¯„ä¼°å¯¹æ•´ä½“è¶‹åŠ¿çš„å½±å“ã€‚")
            
            # ç›‘æµ‹ä¼˜åŒ–å»ºè®®
            if task.interval_hours > 24:
                recommendations.append("**ç›‘æµ‹é¢‘ç‡ä¼˜åŒ–ï¼š** è€ƒè™‘å°†ç›‘æµ‹é—´éš”ä»å½“å‰çš„{}å°æ—¶ç¼©çŸ­è‡³12-24å°æ—¶ï¼Œä»¥è·å–æ›´åŠæ—¶çš„ä¿¡æ¯å’Œè¶‹åŠ¿å˜åŒ–ã€‚".format(task.interval_hours))
            
            # å…³é”®è¯ä¼˜åŒ–å»ºè®®
            keyword_trends = trend_result.get("keyword_trends", {})
            if keyword_trends:
                low_performing = [k for k, v in keyword_trends.items() if v < 3.0]
                if low_performing and len(low_performing) > 1:
                    recommendations.append(f"**å…³é”®è¯ä¼˜åŒ–ï¼š** ä»¥ä¸‹å…³é”®è¯è¡¨ç°è¾ƒå·®ï¼š{', '.join(low_performing[:2])}ã€‚å»ºè®®è€ƒè™‘æ›¿æ¢ä¸ºæ›´ç›¸å…³æˆ–æ›´å…·æ—¶æ•ˆæ€§çš„å…³é”®è¯ã€‚")
            
            # é»˜è®¤å»ºè®®
            if not recommendations:
                recommendations.append("**æŒç»­ä¼˜åŒ–ï¼š** ç»§ç»­ä¿æŒå½“å‰çš„ç›‘æµ‹ç­–ç•¥ï¼Œå®šæœŸè¯„ä¼°å’Œè°ƒæ•´å…³é”®è¯è®¾ç½®ï¼Œå…³æ³¨è¡Œä¸šå‘å±•è¶‹åŠ¿å’Œæ–°å…´è¯é¢˜ã€‚")
            
            return '\n\n'.join(recommendations[:4])  # æœ€å¤šè¿”å›4ä¸ªè¯¦ç»†å»ºè®®
            
        except Exception as e:
            logger.error(f"Comprehensive recommendations generation failed: {e}")
            return "æ­£åœ¨ç”Ÿæˆä¸ªæ€§åŒ–å»ºè®®ï¼Œè¯·ç¨åæŸ¥çœ‹è¯¦ç»†å†…å®¹ã€‚"

    def _extract_detailed_findings(self, research_result: Dict[str, Any], trend_result: Dict[str, Any]) -> List[str]:
        """æå–è¯¦ç»†çš„å…³é”®å‘ç°"""
        try:
            findings = []
            
            # ä»ç ”ç©¶ç»“æœä¸­æå–è¯¦ç»†å‘ç°
            report = research_result.get("report", "")
            if report:
                # å¯»æ‰¾åŒ…å«å…³é”®ä¿¡æ¯çš„å®Œæ•´æ®µè½æˆ–å¥ç¾¤
                paragraphs = [p.strip() for p in report.split('\n\n') if p.strip()]
                if not paragraphs:
                    # å¦‚æœæ²¡æœ‰æ®µè½åˆ†å‰²ï¼Œå°è¯•æŒ‰å¥å·åˆ†å‰²
                    sentences = [s.strip() for s in report.split("ã€‚") if s.strip()]
                    paragraphs = []
                    temp_paragraph = []
                    for sentence in sentences:
                        temp_paragraph.append(sentence)
                        if len("ã€‚".join(temp_paragraph)) > 150:  # ç»„åˆæˆæ®µè½
                            paragraphs.append("ã€‚".join(temp_paragraph) + "ã€‚")
                            temp_paragraph = []
                    if temp_paragraph:  # æ·»åŠ å‰©ä½™å†…å®¹
                        paragraphs.append("ã€‚".join(temp_paragraph) + "ã€‚")
                
                # ç­›é€‰é‡è¦çš„å‘ç°
                important_keywords = ["çªç ´", "åˆ›æ–°", "å‘å±•", "å¢é•¿", "ä¸‹é™", "å˜åŒ–", "è¶‹åŠ¿", "å½±å“", "é‡è¦", "å…³é”®", "æ˜¾è‘—", "ä¸»è¦", "æ–°å…´", "æå‡", "æ”¹å–„", "æŒ‘æˆ˜"]
                
                for paragraph in paragraphs[:15]:  # æ£€æŸ¥å‰15ä¸ªæ®µè½
                    if 30 <= len(paragraph) <= 200:  # é•¿åº¦é€‚ä¸­
                        keyword_count = sum(1 for keyword in important_keywords if keyword in paragraph)
                        if keyword_count >= 1:  # åŒ…å«å…³é”®è¯
                            findings.append(f"**ç ”ç©¶å‘ç°ï¼š** {paragraph}")
                        if len(findings) >= 3:  # é™åˆ¶ç ”ç©¶å‘ç°æ•°é‡
                            break
            
            # ä»è¶‹åŠ¿åˆ†æä¸­æå–è¯¦ç»†ä¿¡æ¯
            trend_score = trend_result.get("trend_score", 5.0)
            activity_level = trend_result.get("activity_level", 5.0)
            
            # æ·»åŠ è¶‹åŠ¿å‘ç°
            if trend_score > 7.5:
                findings.append(f"**è¶‹åŠ¿å‘ç°ï¼š** è¯¥è¯é¢˜å‘ˆç°å¼ºåŠ²ä¸Šå‡è¶‹åŠ¿ï¼ˆè¯„åˆ†: {trend_score:.1f}/10ï¼‰ï¼Œè¡¨æ˜æ­£åœ¨è·å¾—è¶Šæ¥è¶Šå¤šçš„å…³æ³¨å’Œè®¨è®ºï¼Œå¯èƒ½æ­£å¤„äºé‡è¦çš„å‘å±•èŠ‚ç‚¹ã€‚")
            elif trend_score < 3.5:
                findings.append(f"**è¶‹åŠ¿å‘ç°ï¼š** è¯¥è¯é¢˜å‡ºç°ä¸‹é™è¶‹åŠ¿ï¼ˆè¯„åˆ†: {trend_score:.1f}/10ï¼‰ï¼Œå…³æ³¨åº¦å’Œè®¨è®ºçƒ­åº¦æœ‰æ‰€å‡å°‘ï¼Œå¯èƒ½éœ€è¦é‡æ–°è¯„ä¼°å…¶å‘å±•å‰æ™¯æˆ–å¯»æ‰¾æ–°çš„å…³æ³¨è§’åº¦ã€‚")
            
            # æ·»åŠ æ–°å†…å®¹å‘ç°
            new_topics = trend_result.get("new_topics", [])
            emerging_keywords = trend_result.get("emerging_keywords", [])
            
            if new_topics or emerging_keywords:
                new_content = []
                if new_topics:
                    new_content.append(f"æ–°è¯é¢˜: {', '.join(new_topics[:3])}")
                if emerging_keywords:
                    new_content.append(f"æ–°å…³é”®è¯: {', '.join(emerging_keywords[:3])}")
                
                findings.append(f"**æ–°å…´å†…å®¹å‘ç°ï¼š** {'; '.join(new_content)}ã€‚è¿™äº›æ–°å‡ºç°çš„å†…å®¹å¯èƒ½ä»£è¡¨äº†è¯¥é¢†åŸŸçš„æœ€æ–°å‘å±•æ–¹å‘å’Œåˆ›æ–°ç‚¹ï¼Œå€¼å¾—æ·±å…¥å…³æ³¨å’Œåˆ†æã€‚")
            
            # æ·»åŠ å¼‚å¸¸å‘ç°
            if trend_result.get("anomaly_detected"):
                anomaly_desc = trend_result.get("anomaly_description", "æ£€æµ‹åˆ°å¼‚å¸¸å˜åŒ–æ¨¡å¼")
                findings.append(f"**å¼‚å¸¸ç°è±¡å‘ç°ï¼š** {anomaly_desc}ã€‚è¿™ç§å¼‚å¸¸å˜åŒ–å¯èƒ½æ„å‘³ç€é‡è¦çš„è½¬æŠ˜ç‚¹æˆ–çªå‘äº‹ä»¶ï¼Œå»ºè®®è¿›è¡Œæ·±å…¥åˆ†æä»¥äº†è§£å…¶å½±å“å’Œæ„ä¹‰ã€‚")
            
            # æ·»åŠ æƒ…æ„Ÿè¶‹åŠ¿å‘ç°
            sentiment_changes = trend_result.get("sentiment_changes", {})
            significant_sentiment_changes = []
            for sentiment_type, data in sentiment_changes.items():
                if isinstance(data, dict) and abs(data.get("change", 0.0)) > 0.15:
                    direction = "ä¸Šå‡" if data.get("change", 0) > 0 else "ä¸‹é™"
                    change_percent = abs(data.get("change", 0.0)) * 100
                    significant_sentiment_changes.append(f"{sentiment_type}æƒ…æ„Ÿ{direction}{change_percent:.1f}%")
            
            if significant_sentiment_changes:
                findings.append(f"**æƒ…æ„Ÿè¶‹åŠ¿å‘ç°ï¼š** {', '.join(significant_sentiment_changes)}ã€‚è¿™äº›æƒ…æ„Ÿå˜åŒ–åæ˜ äº†å…¬ä¼—å¯¹è¯¥è¯é¢˜æ€åº¦çš„è½¬å˜ï¼Œå¯èƒ½å½±å“å…¶æœªæ¥å‘å±•è½¨è¿¹ã€‚")
            
            return findings[:4]  # æœ€å¤šè¿”å›4ä¸ªè¯¦ç»†å‘ç°
            
        except Exception as e:
            logger.error(f"Detailed findings extraction failed: {e}")
            return ["æ­£åœ¨æ·±å…¥åˆ†ææœ€æ–°æ•°æ®å’Œå‘å±•è¶‹åŠ¿ï¼Œè¯¦ç»†å‘ç°å³å°†ç”Ÿæˆã€‚"]
    
    def _identify_detailed_changes(self, trend_result: Dict[str, Any]) -> List[str]:
        """è¯†åˆ«è¯¦ç»†çš„å…³é”®å˜åŒ–"""
        try:
            changes = []
            
            # è¯¦ç»†çš„æƒ…æ„Ÿå˜åŒ–åˆ†æ
            sentiment_changes = trend_result.get("sentiment_changes", {})
            significant_sentiment_changes = []
            for sentiment_type, data in sentiment_changes.items():
                if isinstance(data, dict):
                    change = data.get("change", 0.0)
                    current = data.get("current", 0.0)
                    historical_avg = data.get("historical_avg", 0.0)
                    trend = data.get("trend", "stable")
                    
                    if abs(change) > 0.1:  # æ˜¾è‘—å˜åŒ–é˜ˆå€¼
                        direction = "ä¸Šå‡" if change > 0 else "ä¸‹é™"
                        change_desc = f"{sentiment_type}æƒ…æ„Ÿ{direction}{abs(change):.1%}"
                        
                        # æ·»åŠ æ›´è¯¦ç»†çš„åˆ†æ
                        if sentiment_type == "positive" and change > 0:
                            analysis = f"æ­£é¢æƒ…ç»ªæ˜¾è‘—ä¸Šå‡ï¼ˆä»{historical_avg:.1%}æå‡è‡³{current:.1%}ï¼‰ï¼Œè¡¨æ˜å…¬ä¼—å¯¹è¯¥è¯é¢˜çš„æ€åº¦è¶‹å‘ç§¯æï¼Œå¯èƒ½åæ˜ äº†å¥½æ¶ˆæ¯æˆ–ç§¯æå‘å±•ã€‚"
                        elif sentiment_type == "negative" and change > 0:
                            analysis = f"è´Ÿé¢æƒ…ç»ªæœ‰æ‰€å¢åŠ ï¼ˆä»{historical_avg:.1%}ä¸Šå‡è‡³{current:.1%}ï¼‰ï¼Œéœ€è¦å…³æ³¨å¯èƒ½å­˜åœ¨çš„é—®é¢˜æˆ–äº‰è®®ï¼Œå»ºè®®æ·±å…¥äº†è§£è´Ÿé¢æƒ…ç»ªçš„æ¥æºã€‚"
                        elif sentiment_type == "positive" and change < 0:
                            analysis = f"æ­£é¢æƒ…ç»ªå‡ºç°ä¸‹æ»‘ï¼ˆä»{historical_avg:.1%}é™è‡³{current:.1%}ï¼‰ï¼Œå¯èƒ½è¡¨æ˜çƒ­åº¦å‡é€€æˆ–å‡ºç°äº†ä¸€äº›ä¸åˆ©å› ç´ ã€‚"
                        elif sentiment_type == "negative" and change < 0:
                            analysis = f"è´Ÿé¢æƒ…ç»ªæœ‰æ‰€ç¼“è§£ï¼ˆä»{historical_avg:.1%}é™è‡³{current:.1%}ï¼‰ï¼Œè¡¨æ˜äº‰è®®æˆ–é—®é¢˜å¾—åˆ°ä¸€å®šç¨‹åº¦çš„è§£å†³æˆ–å…³æ³¨åº¦ä¸‹é™ã€‚"
                        else:
                            analysis = f"{sentiment_type}æƒ…ç»ªå‘ç”Ÿäº†{abs(change):.1%}çš„å˜åŒ–ï¼Œä»{historical_avg:.1%}å˜åŒ–è‡³{current:.1%}ã€‚"
                        
                        changes.append(f"**æƒ…æ„Ÿå˜åŒ–ï¼š** {analysis}")
            
            # è¯¦ç»†çš„è¯é¢˜æ¼”å˜åˆ†æ  
            topic_evolution = trend_result.get("topic_evolution", {})
            new_topics = topic_evolution.get("new_topics", [])
            disappeared_topics = topic_evolution.get("disappeared_topics", [])
            persistent_topics = topic_evolution.get("persistent_topics", [])
            evolution_rate = topic_evolution.get("evolution_rate", 0.0)
            
            if new_topics or disappeared_topics:
                evolution_analysis = []
                if new_topics:
                    evolution_analysis.append(f"æ–°å‡ºç°{len(new_topics)}ä¸ªç›¸å…³è¯é¢˜ï¼š{', '.join(new_topics[:3])}")
                if disappeared_topics:
                    evolution_analysis.append(f"æœ‰{len(disappeared_topics)}ä¸ªè¯é¢˜çƒ­åº¦æ¶ˆé€€ï¼š{', '.join(disappeared_topics[:2])}")
                
                evolution_desc = f"è¯é¢˜æ¼”å˜ç‡ä¸º{evolution_rate:.1%}ï¼Œ"
                if evolution_rate > 0.3:
                    evolution_desc += "è¡¨æ˜è¯¥é¢†åŸŸæ­£ç»å†å¿«é€Ÿå˜åŒ–ï¼Œæ–°çš„å‘å±•æ–¹å‘ä¸æ–­æ¶Œç°ã€‚"
                elif evolution_rate > 0.1:
                    evolution_desc += "è¡¨æ˜è¯¥é¢†åŸŸåœ¨ç¨³æ­¥å‘å±•ï¼Œæœ‰æ–°çš„å…³æ³¨ç‚¹å‡ºç°ã€‚"
                else:
                    evolution_desc += "è¡¨æ˜è¯¥é¢†åŸŸç›¸å¯¹ç¨³å®šï¼Œæ ¸å¿ƒè¯é¢˜ä¿æŒä¸€è‡´ã€‚"
                
                changes.append(f"**è¯é¢˜æ¼”å˜ï¼š** {'; '.join(evolution_analysis)}ã€‚{evolution_desc}")
            
            # è¯¦ç»†çš„å…³é”®è¯è¶‹åŠ¿å˜åŒ–åˆ†æ
            keyword_trends = trend_result.get("keyword_trends", {})
            if keyword_trends:
                trending_up = [(k, v) for k, v in keyword_trends.items() if v > 7.5]
                trending_down = [(k, v) for k, v in keyword_trends.items() if v < 3.5]
                stable_keywords = [(k, v) for k, v in keyword_trends.items() if 4.0 <= v <= 7.0]
                
                if trending_up or trending_down:
                    trend_analysis = []
                    if trending_up:
                        up_keywords = ', '.join([f"{k}({v:.1f})" for k, v in trending_up[:3]])
                        trend_analysis.append(f"çƒ­åº¦ä¸Šå‡çš„å…³é”®è¯ï¼š{up_keywords}ï¼Œè¿™äº›å…³é”®è¯æ­£è·å¾—æ›´å¤šå…³æ³¨")
                    
                    if trending_down:
                        down_keywords = ', '.join([f"{k}({v:.1f})" for k, v in trending_down[:3]])
                        trend_analysis.append(f"çƒ­åº¦ä¸‹é™çš„å…³é”®è¯ï¼š{down_keywords}ï¼Œå¯èƒ½åæ˜ äº†å…³æ³¨ç‚¹çš„è½¬ç§»")
                    
                    stability_desc = ""
                    if stable_keywords:
                        stability_desc = f"åŒæ—¶æœ‰{len(stable_keywords)}ä¸ªå…³é”®è¯ä¿æŒç¨³å®šè¡¨ç°ã€‚"
                    
                    changes.append(f"**å…³é”®è¯è¶‹åŠ¿ï¼š** {'; '.join(trend_analysis)}ã€‚{stability_desc}")
            
            # è¯¦ç»†çš„æ´»è·ƒåº¦å˜åŒ–åˆ†æ
            activity_level = trend_result.get("activity_level", 5.0)
            change_magnitude = trend_result.get("change_magnitude", 0.0)
            
            if activity_level > 8.0 or change_magnitude > 6.0:
                if activity_level > 8.0:
                    activity_desc = f"æ•´ä½“æ´»è·ƒåº¦è¾¾åˆ°é«˜æ°´å¹³ï¼ˆ{activity_level:.1f}/10ï¼‰ï¼Œä¿¡æ¯æ›´æ–°é¢‘ç¹ï¼Œè®¨è®ºçƒ­åº¦å¾ˆé«˜"
                else:
                    activity_desc = f"è™½ç„¶æ´»è·ƒåº¦ä¸º{activity_level:.1f}/10ï¼Œä½†å˜åŒ–å¹…åº¦è¾ƒå¤§ï¼ˆ{change_magnitude:.1f}/10ï¼‰"
                
                impact_analysis = "è¿™ç§é«˜æ´»è·ƒåº¦é€šå¸¸æ„å‘³ç€è¯¥è¯é¢˜æ­£å¤„äºå…³é”®å‘å±•æœŸï¼Œå»ºè®®å¯†åˆ‡å…³æ³¨åç»­åŠ¨æ€ã€‚"
                changes.append(f"**æ´»è·ƒåº¦å˜åŒ–ï¼š** {activity_desc}ã€‚{impact_analysis}")
                
            elif activity_level < 3.0:
                activity_desc = f"æ•´ä½“æ´»è·ƒåº¦è¾ƒä½ï¼ˆ{activity_level:.1f}/10ï¼‰ï¼Œç›¸å…³è®¨è®ºå’Œä¿¡æ¯æ›´æ–°ä¸å¤Ÿé¢‘ç¹"
                suggestion = "å»ºè®®è€ƒè™‘è°ƒæ•´ç›‘æµ‹å…³é”®è¯æˆ–æ‰©å¤§ä¿¡æ¯æºèŒƒå›´ï¼Œä»¥è·å–æ›´å¤šç›¸å…³åŠ¨æ€ã€‚"
                changes.append(f"**æ´»è·ƒåº¦å˜åŒ–ï¼š** {activity_desc}ã€‚{suggestion}")
            
            return changes[:3]  # æœ€å¤šè¿”å›3ä¸ªè¯¦ç»†å˜åŒ–åˆ†æ
            
        except Exception as e:
            logger.error(f"Detailed changes identification failed: {e}")
            return ["æ­£åœ¨æ·±å…¥åˆ†æå˜åŒ–è¶‹åŠ¿ï¼Œè¯¦ç»†åˆ†æå³å°†å®Œæˆã€‚"]
    
    def _generate_trend_summary(self, trend_result: Dict[str, Any]) -> str:
        """ç”Ÿæˆè¶‹åŠ¿æ‘˜è¦"""
        try:
            trend_score = trend_result.get("trend_score", 5.0)
            activity_level = trend_result.get("activity_level", 5.0)
            change_magnitude = trend_result.get("change_magnitude", 0.0)
            confidence_score = trend_result.get("confidence_score", 0.5)
            
            # è¶‹åŠ¿æè¿°
            if trend_score > 7.5:
                trend_desc = "å¼ºåŠ²ä¸Šå‡è¶‹åŠ¿"
            elif trend_score > 6.0:
                trend_desc = "æ¸©å’Œä¸Šå‡è¶‹åŠ¿"
            elif trend_score < 3.5:
                trend_desc = "ä¸‹é™è¶‹åŠ¿"
            elif trend_score < 4.5:
                trend_desc = "æ¸©å’Œä¸‹é™è¶‹åŠ¿"
            else:
                trend_desc = "ç¨³å®šè¶‹åŠ¿"
            
            # æ´»è·ƒåº¦æè¿°
            if activity_level > 7.0:
                activity_desc = "é«˜åº¦æ´»è·ƒ"
            elif activity_level > 5.5:
                activity_desc = "ä¸­ç­‰æ´»è·ƒ"
            else:
                activity_desc = "æ´»è·ƒåº¦ä¸€èˆ¬"
            
            # å˜åŒ–ç¨‹åº¦æè¿°
            if change_magnitude > 7.0:
                change_desc = "å‰§çƒˆå˜åŒ–"
            elif change_magnitude > 4.0:
                change_desc = "æ˜æ˜¾å˜åŒ–"
            else:
                change_desc = "è½»å¾®å˜åŒ–"
            
            # ç½®ä¿¡åº¦æè¿°
            if confidence_score > 0.8:
                confidence_desc = "é«˜ç½®ä¿¡åº¦"
            elif confidence_score > 0.5:
                confidence_desc = "ä¸­ç­‰ç½®ä¿¡åº¦"
            else:
                confidence_desc = "ä½ç½®ä¿¡åº¦"
            
            summary = f"è¶‹åŠ¿è¯„ä¼°: {trend_desc} | æ´»è·ƒç¨‹åº¦: {activity_desc} | å˜åŒ–ç¨‹åº¦: {change_desc} | æ•°æ®å¯é æ€§: {confidence_desc}"
            
            # æ·»åŠ è¯„åˆ†
            summary += f"\nè¯„åˆ†è¯¦æƒ…: è¶‹åŠ¿ {trend_score:.1f}/10, æ´»è·ƒåº¦ {activity_level:.1f}/10, å˜åŒ–å¹…åº¦ {change_magnitude:.1f}/10"
            
            return summary
            
        except Exception as e:
            logger.error(f"Trend summary generation failed: {e}")
            return "è¶‹åŠ¿åˆ†æ: æ•°æ®åˆ†æä¸­ï¼Œè¯·ç¨åæŸ¥çœ‹è¯¦ç»†ç»“æœ"
    
    def _determine_priority_level(self, trend_result: Dict[str, Any]) -> str:
        """ç¡®å®šä¼˜å…ˆçº§çº§åˆ«"""
        try:
            trend_score = trend_result.get("trend_score", 5.0)
            change_magnitude = trend_result.get("change_magnitude", 0.0)
            anomaly_detected = trend_result.get("anomaly_detected", False)
            
            # å¼‚å¸¸æ£€æµ‹åˆ°ï¼Œé«˜ä¼˜å…ˆçº§
            if anomaly_detected:
                return "high"
            
            # é«˜è¶‹åŠ¿åˆ†æ•°æˆ–å¤§å˜åŒ–å¹…åº¦
            if trend_score > 8.0 or change_magnitude > 7.0:
                return "high"
            
            # ä½è¶‹åŠ¿åˆ†æ•°ï¼Œä¸­ä¼˜å…ˆçº§
            if trend_score < 3.0:
                return "medium"
            
            # æœ‰ä¸€å®šå˜åŒ–ï¼Œä¸­ä¼˜å…ˆçº§
            if change_magnitude > 4.0:
                return "medium"
            
            # å…¶ä»–æƒ…å†µï¼Œä½ä¼˜å…ˆçº§
            return "low"
            
        except Exception as e:
            logger.error(f"Priority level determination failed: {e}")
            return "medium"
    
    def _generate_action_recommendations(self, task, trend_result: Dict[str, Any]) -> List[str]:
        """ç”Ÿæˆè¡ŒåŠ¨å»ºè®®"""
        try:
            recommendations = []
            
            trend_score = trend_result.get("trend_score", 5.0)
            activity_level = trend_result.get("activity_level", 5.0)
            new_topics = trend_result.get("new_topics", [])
            anomaly_detected = trend_result.get("anomaly_detected", False)
            
            # åŸºäºè¶‹åŠ¿åˆ†æ•°çš„å»ºè®®
            if trend_score > 8.0:
                recommendations.append("å¯†åˆ‡å…³æ³¨å½“å‰å‘å±•ï¼Œè€ƒè™‘åŠ å¤§æŠ•å…¥æˆ–å…³æ³¨åŠ›åº¦")
            elif trend_score < 3.0:
                recommendations.append("è¯„ä¼°æ˜¯å¦éœ€è¦è°ƒæ•´ç­–ç•¥æˆ–é‡æ–°å®šä½å…³æ³¨é‡ç‚¹")
            
            # åŸºäºæ´»è·ƒåº¦çš„å»ºè®®
            if activity_level > 8.0:
                recommendations.append("å½“å‰æ´»è·ƒåº¦å¾ˆé«˜ï¼Œå»ºè®®åŠæ—¶è·Ÿè¿›æœ€æ–°åŠ¨æ€")
            elif activity_level < 3.0:
                recommendations.append("æ´»è·ƒåº¦è¾ƒä½ï¼Œå¯èƒ½éœ€è¦å¯»æ‰¾æ–°çš„ä¿¡æ¯æºæˆ–è°ƒæ•´å…³é”®è¯")
            
            # åŸºäºæ–°è¯é¢˜çš„å»ºè®®
            if len(new_topics) > 3:
                recommendations.append(f"å‡ºç°å¤šä¸ªæ–°è¯é¢˜ï¼Œå»ºè®®æ·±å…¥ç ”ç©¶: {', '.join(new_topics[:2])}")
            
            # åŸºäºå¼‚å¸¸çš„å»ºè®®
            if anomaly_detected:
                recommendations.append("æ£€æµ‹åˆ°å¼‚å¸¸å˜åŒ–ï¼Œå»ºè®®ç«‹å³è¿›è¡Œæ·±åº¦åˆ†æ")
            
            # åŸºäºä»»åŠ¡é…ç½®çš„å»ºè®®
            if task.interval_hours > 24:
                recommendations.append("è€ƒè™‘ç¼©çŸ­ç›‘æµ‹é—´éš”ä»¥è·å–æ›´åŠæ—¶çš„ä¿¡æ¯")
            
            # é»˜è®¤å»ºè®®
            if not recommendations:
                recommendations.append("ç»§ç»­å®šæœŸç›‘æµ‹ï¼Œå…³æ³¨è¶‹åŠ¿å˜åŒ–")
            
            return recommendations[:3]  # æœ€å¤šè¿”å›3ä¸ªå»ºè®®
            
        except Exception as e:
            logger.error(f"Action recommendations generation failed: {e}")
            return ["ç»§ç»­ç›‘æµ‹è¯é¢˜å‘å±•è¶‹åŠ¿"]
    
    def _suggest_next_focus_areas(self, trend_result: Dict[str, Any]) -> List[str]:
        """æ¨èä¸‹ä¸€æ­¥å…³æ³¨é¢†åŸŸ"""
        try:
            focus_areas = []
            
            # åŸºäºæ–°è¯é¢˜
            new_topics = trend_result.get("new_topics", [])
            if new_topics:
                focus_areas.extend(new_topics[:2])
            
            # åŸºäºæ–°å…´å…³é”®è¯
            emerging_keywords = trend_result.get("emerging_keywords", [])
            if emerging_keywords:
                focus_areas.extend(emerging_keywords[:2])
            
            # åŸºäºè¶‹åŠ¿ä¸Šå‡çš„å…³é”®è¯
            keyword_trends = trend_result.get("keyword_trends", {})
            trending_keywords = [k for k, v in keyword_trends.items() if v > 7.5]
            if trending_keywords:
                focus_areas.extend(trending_keywords[:2])
            
            # å»é‡å¹¶é™åˆ¶æ•°é‡
            unique_focus_areas = list(dict.fromkeys(focus_areas))  # ä¿æŒé¡ºåºçš„å»é‡
            
            return unique_focus_areas[:4]  # æœ€å¤šè¿”å›4ä¸ªå…³æ³¨é¢†åŸŸ
            
        except Exception as e:
            logger.error(f"Next focus areas suggestion failed: {e}")
            return ["ç›¸å…³æŠ€æœ¯å‘å±•", "å¸‚åœºåŠ¨æ€å˜åŒ–"]
    
    def _create_fallback_summary(self, research_result: Dict[str, Any], task) -> Dict[str, Any]:
        """åˆ›å»ºåå¤‡æ‘˜è¦ï¼ˆå‡ºé”™æ—¶ï¼‰"""
        report = research_result.get("report", "")
        
        # ç®€å•æå–å‰å‡ å¥ä½œä¸ºæ‘˜è¦
        sentences = report.split("ã€‚")[:3]
        simple_summary = "ã€‚".join(sentences) + "ã€‚" if sentences else f"å…³äº {task.topic} çš„ç ”ç©¶åˆ†æå·²å®Œæˆã€‚"
        
        return {
            "summary": simple_summary,
            "key_findings": ["æ­£åœ¨æ·±å…¥åˆ†ææœ€æ–°æ•°æ®ï¼Œè¯¦ç»†å‘ç°å³å°†ç”Ÿæˆ"],
            "key_changes": ["æ­£åœ¨ç›‘æµ‹å’Œåˆ†æå…³é”®å˜åŒ–è¶‹åŠ¿"]
        }
